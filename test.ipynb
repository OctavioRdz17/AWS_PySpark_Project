{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configurar el registro de eventos\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "os_input_s3_cleansed_layer = 's3://de-holyproject-cleansed-useast1-dev/clean_statistics_reference_data/'\n",
    "os_input_glue_catalog_db_name = 'de_holyproject_cleaned'\n",
    "os_input_glue_catalog_table_name = 'cleaned_statistics_reference_data'\n",
    "os_input_write_data_operation = 'append'\n",
    "\n",
    "bucket = 'de-holyproject-raw-useast1-dev'\n",
    "key = 'raw_statistics_reference_data/CA_category_id.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing object raw_statistics_reference_data/CA_category_id.json from bucket de-holyproject-raw-useast1-dev: The specified path: s3://de-holyproject-cleansed-useast1-dev/clean_statistics_reference_data/, does not match the existing Glue catalog table path: s3://de-holyproject-cleansed-useast1-dev/youtube\n"
     ]
    },
    {
     "ename": "InvalidArgumentValue",
     "evalue": "The specified path: s3://de-holyproject-cleansed-useast1-dev/clean_statistics_reference_data/, does not match the existing Glue catalog table path: s3://de-holyproject-cleansed-useast1-dev/youtube",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentValue\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32md:\\Proyectos\\AWS_PySpark_Project\\test.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError processing object \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m from bucket \u001b[39m\u001b[39m{\u001b[39;00mbucket\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Rethrow the exception to maintain Lambda error handling\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[1;32md:\\Proyectos\\AWS_PySpark_Project\\test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     df_step_1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mjson_normalize(df_raw[\u001b[39m'\u001b[39m\u001b[39mitems\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# print(df_step_1.head(3))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# Write to S3\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     wr_response \u001b[39m=\u001b[39m wr\u001b[39m.\u001b[39;49ms3\u001b[39m.\u001b[39;49mto_parquet(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         df\u001b[39m=\u001b[39;49mdf_step_1,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         path\u001b[39m=\u001b[39;49mos_input_s3_cleansed_layer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         database\u001b[39m=\u001b[39;49mos_input_glue_catalog_db_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         table\u001b[39m=\u001b[39;49mos_input_glue_catalog_table_name,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         mode\u001b[39m=\u001b[39;49mos_input_write_data_operation\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(wr_response)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Proyectos/AWS_PySpark_Project/test.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Log the error message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\awswrangler\\_config.py:733\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[1;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[39mdel\u001b[39;00m args[name]\n\u001b[0;32m    732\u001b[0m         args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeywords}\n\u001b[1;32m--> 733\u001b[0m \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\awswrangler\\_utils.py:176\u001b[0m, in \u001b[0;36mvalidate_kwargs.<locals>.decorator.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m condition_fn() \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(passed_unsupported_kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    174\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mInvalidArgument(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmessage\u001b[39m}\u001b[39;00m\u001b[39m `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(passed_unsupported_kwargs)\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 176\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\awswrangler\\s3\\_write_parquet.py:717\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, index, compression, pyarrow_additional_kwargs, max_rows_by_file, use_threads, boto3_session, s3_additional_kwargs, sanitize_columns, dataset, filename_prefix, partition_cols, bucketing_info, concurrent_partitioning, mode, catalog_versioning, schema_evolution, database, table, glue_table_settings, dtype, athena_partition_projection_settings, catalog_id)\u001b[0m\n\u001b[0;32m    714\u001b[0m     pyarrow_additional_kwargs[\u001b[39m\"\u001b[39m\u001b[39mflavor\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspark\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    716\u001b[0m strategy \u001b[39m=\u001b[39m _S3ParquetWriteStrategy()\n\u001b[1;32m--> 717\u001b[0m \u001b[39mreturn\u001b[39;00m strategy\u001b[39m.\u001b[39;49mwrite(\n\u001b[0;32m    718\u001b[0m     df\u001b[39m=\u001b[39;49mdf,\n\u001b[0;32m    719\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    720\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m    721\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    722\u001b[0m     pyarrow_additional_kwargs\u001b[39m=\u001b[39;49mpyarrow_additional_kwargs,\n\u001b[0;32m    723\u001b[0m     max_rows_by_file\u001b[39m=\u001b[39;49mmax_rows_by_file,\n\u001b[0;32m    724\u001b[0m     use_threads\u001b[39m=\u001b[39;49muse_threads,\n\u001b[0;32m    725\u001b[0m     boto3_session\u001b[39m=\u001b[39;49mboto3_session,\n\u001b[0;32m    726\u001b[0m     s3_additional_kwargs\u001b[39m=\u001b[39;49ms3_additional_kwargs,\n\u001b[0;32m    727\u001b[0m     sanitize_columns\u001b[39m=\u001b[39;49msanitize_columns,\n\u001b[0;32m    728\u001b[0m     dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[0;32m    729\u001b[0m     filename_prefix\u001b[39m=\u001b[39;49mfilename_prefix,\n\u001b[0;32m    730\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[0;32m    731\u001b[0m     bucketing_info\u001b[39m=\u001b[39;49mbucketing_info,\n\u001b[0;32m    732\u001b[0m     concurrent_partitioning\u001b[39m=\u001b[39;49mconcurrent_partitioning,\n\u001b[0;32m    733\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m    734\u001b[0m     catalog_versioning\u001b[39m=\u001b[39;49mcatalog_versioning,\n\u001b[0;32m    735\u001b[0m     schema_evolution\u001b[39m=\u001b[39;49mschema_evolution,\n\u001b[0;32m    736\u001b[0m     database\u001b[39m=\u001b[39;49mdatabase,\n\u001b[0;32m    737\u001b[0m     table\u001b[39m=\u001b[39;49mtable,\n\u001b[0;32m    738\u001b[0m     description\u001b[39m=\u001b[39;49mdescription,\n\u001b[0;32m    739\u001b[0m     parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    740\u001b[0m     columns_comments\u001b[39m=\u001b[39;49mcolumns_comments,\n\u001b[0;32m    741\u001b[0m     table_type\u001b[39m=\u001b[39;49mtable_type,\n\u001b[0;32m    742\u001b[0m     transaction_id\u001b[39m=\u001b[39;49mtransaction_id,\n\u001b[0;32m    743\u001b[0m     regular_partitions\u001b[39m=\u001b[39;49mregular_partitions,\n\u001b[0;32m    744\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    745\u001b[0m     athena_partition_projection_settings\u001b[39m=\u001b[39;49mathena_partition_projection_settings,\n\u001b[0;32m    746\u001b[0m     catalog_id\u001b[39m=\u001b[39;49mcatalog_id,\n\u001b[0;32m    747\u001b[0m     compression_ext\u001b[39m=\u001b[39;49mcompression_ext,\n\u001b[0;32m    748\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\octav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\awswrangler\\s3\\_write.py:315\u001b[0m, in \u001b[0;36m_S3WriteStrategy.write\u001b[1;34m(self, df, path, index, compression, pyarrow_additional_kwargs, max_rows_by_file, use_threads, boto3_session, s3_additional_kwargs, sanitize_columns, dataset, filename_prefix, partition_cols, bucketing_info, concurrent_partitioning, mode, catalog_versioning, schema_evolution, database, table, description, parameters, columns_comments, transaction_id, regular_partitions, table_type, dtype, athena_partition_projection_settings, catalog_id, compression_ext)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mand\u001b[39;00m catalog_path:\n\u001b[0;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m!=\u001b[39m catalog_path\u001b[39m.\u001b[39mrstrip(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 315\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mInvalidArgumentValue(\n\u001b[0;32m    316\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe specified path: \u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m, does not match the existing Glue catalog table path: \u001b[39m\u001b[39m{\u001b[39;00mcatalog_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    317\u001b[0m         )\n\u001b[0;32m    319\u001b[0m \u001b[39mif\u001b[39;00m (table_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGOVERNED\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m transaction_id):\n\u001b[0;32m    320\u001b[0m     _logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m`transaction_id` not specified for GOVERNED table, starting transaction\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mInvalidArgumentValue\u001b[0m: The specified path: s3://de-holyproject-cleansed-useast1-dev/clean_statistics_reference_data/, does not match the existing Glue catalog table path: s3://de-holyproject-cleansed-useast1-dev/youtube"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Creating DF from content\n",
    "    df_raw = wr.s3.read_json('s3://{}/{}'.format(bucket, key))\n",
    "\n",
    "    # Extract required columns:\n",
    "    df_step_1 = pd.json_normalize(df_raw['items'])\n",
    "\n",
    "    # print(df_step_1.head(3))\n",
    "\n",
    "    # Write to S3\n",
    "    wr_response = wr.s3.to_parquet(\n",
    "        df=df_step_1,\n",
    "        path=os_input_s3_cleansed_layer,\n",
    "        dataset=True,\n",
    "        database=os_input_glue_catalog_db_name,\n",
    "        table=os_input_glue_catalog_table_name,\n",
    "        mode=os_input_write_data_operation\n",
    "    )\n",
    "    print(wr_response)\n",
    "\n",
    "except Exception as e:\n",
    "    # Log the error message\n",
    "    logger.error(f'Error processing object {key} from bucket {bucket}: {str(e)}')\n",
    "\n",
    "    # Rethrow the exception to maintain Lambda error handling\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
